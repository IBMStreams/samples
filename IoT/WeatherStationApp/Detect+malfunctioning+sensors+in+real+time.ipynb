{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect potentially malfunctioning sensors in real time using Streaming Analytics and Python\n",
    "\n",
    "\n",
    "In this notebook, you will create an application that receives weather data from simulated weather stations and then detects if any of those stations are malfunctioning.  This is done by comparing the temperature from each station with the average temperature from all the other stations in the same region. If a station's reading is considered to be an outlier, then it is flagged as potentially malfunctioning.\n",
    "\n",
    "The data is visualized on a map, and malfunctioning stations are shown as red, as in the image below. Note that the readings are updated in real time.\n",
    "<img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/09/dsx-weather-app.gif\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup\n",
    "\n",
    "\n",
    "This notebook requires you to have the Streaming Analytics service and the Watson IoT Platform service. You can set up and connect the services manually or use the automatic option if it applies to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Option 1: Automatically deploy and configure the services\n",
    "\n",
    "You can use this option if \n",
    "- You do not have the services service created in Bluemix, or\n",
    "- You have both services create and would like to use automatic configuration. If so, you need to rename the services to match the names expected by the configuration script before starting the process:\n",
    "   - Rename the Streaming Analtyics service to *Streaming-Analytics* \n",
    "   - Rename the Watson IoT Platform to *Internet-of-Things-Platform*.\n",
    "   **These names must match exactly as indicated here**.\n",
    "\n",
    "[![Deploy To Bluemix](https://bluemix.net/deploy/button.png)](https://bluemix.net/deploy?repository=https://github.com/natashadsilva/streamsx.iot.starter.git)\n",
    "Click the \"Deploy To Bluemix\" button above to deploy the services automatically.\n",
    "\n",
    "Once the deployment is finished, go to your Bluemix dashboard as shown below and click the link to go to the home page of the starter kit. This is where you'll access the needed credentials.\n",
    "\n",
    "![Landing page](https://raw.githubusercontent.com/natashadsilva/streamsx.iot.starter/master/bluemix-dashboard.png)\n",
    "\n",
    "\n",
    "### Finish setup\n",
    "#### Go to the home page of the starter kit:\n",
    "1. Go to \"Tools\" and click \"Submit IotPlatform Job\".\n",
    "1. Download the `device.cfg` file: Click **View All Credentials**, then under **Edgent Credentials**, select **Download Device.cfg**. Save this file locally.\n",
    "2. Get credentials for your Streaming Analytics service:\n",
    " - From the landing page, click **View All Credentials**.\n",
    " - In the  **Streams credentials** tab, click \"Get Credentials\", and copy the contents of the cell. You will paste them where required in the cell below. \n",
    " - Paste them in the cell titled *Specify Streams Credentials* below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Option 2: Manual deployment \n",
    "\n",
    "If you have not already done so, create instances of the [Streaming Analytics service](https://console.bluemix.net/catalog/services/streaming-analytics) and the [Watson IoT Platform](https://console.bluemix.net/catalog/services/internet-of-things-platform) service. \n",
    "\n",
    "Next, follow the [instructions in this post](https://developer.ibm.com/streamsdev/docs/setup-instructions-connecting-edgent-streams-applications-watson-iot-platform)\n",
    "After completing the steps in the above article, you should have:\n",
    " - A registered device on the Watson IoT Platform and a `device.cfg` file with the information for the device.\n",
    " - The `IotPlatformBluemix` application running in your Streaming Analytics application.\n",
    " \n",
    "Finally, **get your Streaming Analytics credentials** and paste them in the cell below:\n",
    "- Open the Streaming Analytics service dashboard, click **Service Credentials**. If no credentials are listed, click **New Credential** to create a set of credentials. Then click **View Credentials**, and finally click the Copy icon.\n",
    "- You need the *service_name* (`test1` in the screenshot)  below. Then, set  the credentials you copied to the `credentials` variable in the cell below and change the `service_name` variable to your service name.\n",
    "\n",
    "<img src='https://github.com/orzade/streamsx-notebooks/blob/master/copyservicecredentials.png?raw=true' alt=\"Copy your service credentials\" title=\"Streaming Analytics catalog - Copy your service credentials\"></img>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Specify Streams credentials\n",
    "- Set  the Streaming Analytics credentials you copied to the `credentials` variable in the cell below.\n",
    "- Change the `service_name` variable to your service name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up access to Streaming Analytics service\n",
    "\n",
    "def get_service_name():\n",
    "    service_name = \"Streaming-Analytics\" ## If you chose manual deployment, change the service name here \n",
    "    return service_name\n",
    "def get_credentials():\n",
    "    \n",
    "    credentials = \"\"\" Paste retrieved credentials here \"\"\"\n",
    "    \n",
    "      \n",
    "    return credentials\n",
    "\n",
    "def submit_to_service(topo):\n",
    "    service_name = get_service_name()\n",
    "    credentials = get_credentials()\n",
    "    vs={'streaming-analytics': [{'name': service_name, 'credentials': json.loads (credentials)}]}\n",
    "    cfg = {}\n",
    "    cfg[ConfigParams.VCAP_SERVICES] = vs\n",
    "    cfg[ConfigParams.SERVICE_NAME] = service_name\n",
    "    return submit('STREAMING_ANALYTICS_SERVICE', topo, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Start generating data\n",
    "The data processed by this notebook is from an Edgent application that sends simulated weather data from different locations in Toronto and Markham.  \n",
    "Download and unpack the [Weather Station simulator application](https://github.com/IBMStreams/samples/raw/WeatherStationApp/IoT/WeatherStationApp/WeatherStationSimulator/weather-station-simulator.jar).\n",
    "\n",
    "Make sure you have also saved your `device.cfg` file locally.\n",
    "\n",
    "Start generating data by running the application:\n",
    "\n",
    "`java -Dcom.ibm.iotf.enableCustomFormat=false -jar weather-station-simulator.jar path/to/device.cfg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2. Create Streams application\n",
    "\n",
    "This application will ingest data from the sensors in different locations and show the live readings from each sensor on a map, updating in real time. It will also show any sensors that are detected to be outliers as red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define helper classes\n",
    "\n",
    "The `TagOutliers` class below will compute the rolling average temperature for the weather stations, and then will add a tag to each station based on whether or not it could possibly be malfunctioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "        \n",
    "def parse_json(tuple):\n",
    "    js = tuple[\"jsonString\"]\n",
    "    reading = json.loads(js)\n",
    "    return reading[\"d\"]\n",
    "   \n",
    "class TagOutliers():\n",
    "    \"\"\"\n",
    "    A callable class that determines if a tuple is an outlier. \n",
    "    It adds a new key \"outlier\" to the tuple to indicate whether or not the value is an outlier.\n",
    "    An outlier is defined as more than (threshold * standard deviation) from the average.\n",
    "    \n",
    "    Args:\n",
    "        threshold (number): threshold\n",
    "        n: window size, the number of items used to compute the average\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold, n):\n",
    "        self.threshold = threshold\n",
    "        self.window = {}\n",
    "        self.num = n\n",
    "    def window_isfull(self):\n",
    "        return len(self.window) == self.num \n",
    "    def is_outlier(self, value, average, stddev):\n",
    "        distance_from_avg = abs(value-average)\n",
    "        #if the distance_from_avg exceeds the threshold then this is an outlier\n",
    "        is_outlier = distance_from_avg > (self.threshold *stddev)\n",
    "        return is_outlier\n",
    "    \n",
    "    def __call__(self, tuple):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tuple that represents a reading from a weather station.\n",
    "        Returns:\n",
    "            None if the window is not full.\n",
    "            When the window is full, returns a list of all the tuples in the window, flagging those which are outliers\n",
    "        \"\"\"\n",
    "        #add an entry to the window for this tuple based on its id\n",
    "        self.window[tuple[\"id\"]] = tuple\n",
    "        \n",
    "        #calculate stddev and avg when the window is full\n",
    "        if self.window_isfull():\n",
    "\n",
    "            readings = [x[\"temp\"] for x in self.window.values()]\n",
    "            #use numpy.average and std to compute average and stddev\n",
    "            avg = np.average(readings);\n",
    "            stddev = np.std(readings)\n",
    "            #determine which of the stations in the window have a value that is an outlier\n",
    "            for station in self.window.values():\n",
    "                temp = station[\"temp\"]\n",
    "                station[\"outlier\"] = str(self.is_outlier(temp, avg, stddev))\n",
    "                station[\"avg\"] = avg\n",
    "                \n",
    "            values =  list(self.window.values()) \n",
    "            self.window = {}\n",
    "            \n",
    "            return values\n",
    "\n",
    "    \n",
    "class MergeStreams:\n",
    "    \"\"\"\n",
    "    It will take two streams of lists, and produce one stream with the contents of lists\n",
    "    The incoming data is split by location, and this class merges the previously split streams\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.last_rh = []\n",
    "        self.last_mkm = []\n",
    "    \n",
    "    def __call__(self, tuple):\n",
    "        if tuple[0][\"location\"] == \"tor\":\n",
    "            self.last_rh = tuple\n",
    "        else:\n",
    "            self.last_mkm = tuple\n",
    "        if len(self.last_mkm) > 0 and len(self.last_rh) > 0 :\n",
    "            merged = self.last_mkm + self.last_rh\n",
    "            self.last_mkm = []\n",
    "            self.last_rh = []\n",
    "            return merged\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Streams `Topology` to tag outliers\n",
    "\n",
    "Streams applications are directed graphs with data moving from one operation to the next. A Streams application written in Python is called a `Topology`.\n",
    "The `Topology` we are creating will:\n",
    "- Subscribe to data from the Watson IoT platform, \n",
    "- Create 2  groups of readings, one for each location so we can compare each sensor to its nearest neighbours.\n",
    "- Use numpy to determine the rolling average and standard deviation and check each sensor's values to see if it is an outlier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 24, 2017 6:30:58 PM com.ibm.streamsx.topology.internal.context.remote.BuildServiceRemoteRESTWrapper remoteBuildAndSubmit\n",
      "INFO: Streaming Analytics service (Streaming-Analytics): Checking status\n",
      "Oct 24, 2017 6:31:02 PM com.ibm.streamsx.topology.internal.streaminganalytics.RestUtils checkInstanceStatus\n",
      "INFO: Streaming Analytics service (Streaming-Analytics): instance status response:{\"state\":\"STARTED\",\"plan\":\"Standard\",\"enabled\":true,\"status\":\"running\"}\n",
      "Oct 24, 2017 6:31:02 PM com.ibm.streamsx.topology.internal.context.remote.BuildServiceRemoteRESTWrapper remoteBuildAndSubmit\n",
      "INFO: Streaming Analytics service (Streaming-Analytics): submitting build TaggedWeatherStationData_6799C37D92D5A3E5\n",
      "Oct 24, 2017 6:32:20 PM com.ibm.streamsx.topology.internal.context.remote.BuildServiceRemoteRESTWrapper remoteBuildAndSubmit\n",
      "INFO: Streaming Analytics service (Streaming-Analytics): submitting job request.\n",
      "Oct 24, 2017 6:32:25 PM com.ibm.streamsx.topology.internal.context.remote.BuildServiceRemoteRESTWrapper doSubmitJobFromBuildArtifactPut\n",
      "INFO: Streaming Analytics service (Streaming-Analytics): submit job response: {\"artifact\":\"16436\",\"jobId\":\"11\",\"application\":\"ipythoninput5cb63b5b124a5::TaggedWeatherStationData\",\"name\":\"ipythoninput5cb63b5b124a5::TaggedWeatherStationData_11\",\"state\":\"STARTED\",\"plan\":\"Standard\",\"enabled\":true,\"status\":\"running\"}\n",
      "\n",
      "Submitted job to Streaming Analytics service\n"
     ]
    }
   ],
   "source": [
    "#this window size is the number of stations in each location, \n",
    "#if you change the Edgent application to add more stations, this needs to be changed accordingly.\n",
    "from streamsx.topology.topology import Topology\n",
    "from streamsx.topology.context import *\n",
    "from streamsx.topology import schema\n",
    "import streamsx.spl.op as op\n",
    "import json\n",
    "\n",
    "NUM_STATIONS = 20\n",
    "NUM_STATIONS_PER_LOCATION = NUM_STATIONS/2\n",
    "window_size = NUM_STATIONS_PER_LOCATION\n",
    "threshold = 2\n",
    "\n",
    "# Create Topology - our application graph\n",
    "topo = Topology('TaggedWeatherStationData')\n",
    "sch =  schema.StreamSchema(\"tuple <rstring typeId, rstring deviceId, rstring eventId,rstring jsonString>\")\n",
    "# read from data source\n",
    "raw_events = topo.subscribe(topic=\"streamsx/iot/device/events\", schema=sch,name= \"IoTEvents\")\n",
    "\n",
    "# Get only events with id \"weather\"\n",
    "json_data_from_iotp = raw_events.filter(lambda tuple: tuple[\"eventId\"] == \"weather\", \"RawEvents\")\n",
    "\n",
    "#parse json to python objects\n",
    "readings = json_data_from_iotp.map(parse_json,\"WeatherEvents\")\n",
    "\n",
    "# Split by location so that averages are computed based on location\n",
    "tor_str = readings.filter(lambda tuple: tuple[\"location\"] == \"tor\", \"Toronto\")\n",
    "mkm_str  = readings.filter(lambda tuple: tuple[\"location\"] == \"mkm\", \"Markham\")\n",
    "\n",
    "#each stream is a list of all the current readings for each station\n",
    "#the TagOutliers class uses numpy to compute average and standard deviation and determines which stations are outliers\n",
    "tor_tagged =  tor_str.map(TagOutliers(threshold, window_size))\n",
    "mkm_tagged = mkm_str.map(TagOutliers(threshold, window_size))\n",
    "\n",
    "#merge the output streams for easy graphing in plotly\n",
    "merged = tor_tagged.union({mkm_tagged}).map(MergeStreams())\n",
    "\n",
    "merged.print()\n",
    "\n",
    "#this view allows us to graph the data from the merged stream\n",
    "station_data_view = merged.view()\n",
    "\n",
    "# Submit on Bluemix\n",
    "job_submission_result = submit_to_service(topo)\n",
    "\n",
    "print(\"\\nSubmitted job to Streaming Analytics service\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 View the data in the Streams Console\n",
    "Now that your application is running, you can view the printed output in the Streams console.\n",
    "\n",
    "- Go to the [Bluemix dashboard](https://console.bluemix.net/dashboard/) \n",
    "- Click your Streaming Analytics service instance\n",
    "- Click launch on the service management page. This will open the Streams Console.\n",
    "- Click the \"Log Viewer\", and look for the application with a name of the form `ipythoninput::TaggedWeatherStations`, and open the **Console Log** as shown below. You can see the output of the application.\n",
    "![ConsoleLog](https://github.com/natashadsilva/streamsx.iot.starter/raw/master/view_log.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3: Plot the Weather Station Data\n",
    "\n",
    "We now have a Streams application running and ingesting data, and we've seen its output in the Streams console.  But this is just printed data. We would like to take the output of the Streams application and display it on a Plotly map so we can see which weather stations are malfunctioning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get your Plotly credentials information\n",
    "\n",
    "We want to create a map that shows readings from weather stations in real time\n",
    "\n",
    "This map is created using Plotly. \n",
    "So, if you want to create the visualization, you need to [register with Plotly](https://plot.ly/accounts/login/?action=signup) then get the following keys for your Plotly account:\n",
    "- API key  \n",
    "- Streaming API key.\n",
    "\n",
    "Paste that informatin in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change the values in this cell with your username, api key, stream ids\n",
    "\n",
    "plotly_api_key = 'abcdsf'\n",
    "plotly_stream_ids =['ahighjam9','12324xxf']\n",
    "plotly_username = 'yourplotlyid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Setup the Plotly map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.1 Create Plotly stream  objects using your credentials\n",
    "\n",
    "`plotly_stream_id` object is the object that identifies the stream. We will also create a plotly stream link object called `plotly_data_stream`. The stream link object is used to update the map with streaming data. Both objects require the `plotly_stream_id` you just set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotly.graph_objs import *\n",
    "import plotly\n",
    "import plotly.plotly as pty\n",
    "import plotly.tools \n",
    "import plotly.graph_objs as graph_objs\n",
    "\n",
    "\n",
    "def get_trace():\n",
    "    loc_1_lats = [43.722,43.685,43.671,43.677,43.673]\n",
    "    loc_1_lons = [-79.384,-79.474,-79.343,-79.421,-79.345]\n",
    "    loc_2_lats  =[43.870,43.880,43.844750,43.8570,43.830]\n",
    "    loc_2_lons  = [-79.271,-79.362,-79.330,-79.370,-79.310]\n",
    "    loc_ids = [\"loc_\" + str(x) for x in range(20)]\n",
    "    lats = loc_1_lats + loc_2_lats\n",
    "    lons=loc_1_lons+loc_2_lons\n",
    "    trace = Data([\n",
    "        Scattermapbox(lat= lats,lon=lons,\n",
    "            mode='markers+text',\n",
    "            marker=Marker(\n",
    "                size=40,symbol=\"circle\",color=\"royalblue\",\n",
    "            ),ids=loc_ids,hovertext=\"\", text=loc_ids,stream=plotly_stream_id, textfont=dict(size=\"12\",color=\"white\")\n",
    "        )\n",
    "    ])\n",
    "    return trace\n",
    "\n",
    "def get_layout():\n",
    "    layout = Layout(autosize=True, hovermode='closest' ,title=\"Weather Stations\",\n",
    "        mapbox= dict(bearing=0,  pitch=0, zoom=10,\n",
    "            center=dict(lat=43.760859080766361,\n",
    "                        lon=-79.3474682425380699)),\n",
    "    )\n",
    "    return layout\n",
    "\n",
    "#Configure plotly to use our token\n",
    "plotly.tools.set_credentials_file(username=plotly_username, api_key=plotly_api_key, stream_ids=plotly_stream_ids)\n",
    "\n",
    "stream_tokens = plotly.tools.get_credentials_file()['stream_ids']\n",
    "#In our case we just have the one token. If you wanted to graph multiple streams you would have to use multiple tokens.\n",
    "token = stream_tokens[0]   \n",
    "\n",
    "plotly_stream_id = dict(token=token, maxpoints=100)\n",
    "plotly_data_stream = pty.Stream(stream_id=token)\n",
    "\n",
    "\n",
    "fig = dict(data=get_trace(), layout=get_layout())\n",
    "map_url = pty.plot( fig, validate=False, filename='weather_stn_graph', auto_open=False, fileopt='overwrite')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Fetch and display the sensor data from Streams\n",
    "Now, we can start streaming data from Streams to our map.  Here we will define the `send_data_to_plotly` function will retrieve the tagged weather station data from Streams and send it to the plotly map above using the stream link object `plotly_data_stream`. Since this is a potentially indefinite stream, I use the `num_refreshes` parameter to control how many times the data is fetched.  We will call this function when we are ready to view the map.\n",
    "\n",
    "If the map is no longer updating, re-run the cell below.\n",
    "\n",
    "\n",
    "Below we'll put the code to retrieve and visualize the data in a function we call `send_data_to_plotly()`. As the name implies, it will retrieve data from Streams,and send it to our Plotly map. Though this function could run indefinitely, we will set it to run only for a few minutes. You can re-run the cell to start generating data once it stops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def send_data_to_plotly(streams_view, plotly_map_stream, num_refreshes, tag=True):\n",
    "    plotly_map_stream.open() \n",
    "    view_data = streams_view.start_data_fetch()\n",
    "   \n",
    "    for i in range(num_refreshes):\n",
    "        try:\n",
    "            stations = view_data.get()\n",
    "            #need list of latitudes, longitudes\n",
    "            lats =[]\n",
    "            ids =[]\n",
    "            lons = []\n",
    "            colors = []\n",
    "            labels=[]\n",
    "            #print(stations)\n",
    "            for station in stations:\n",
    "                lats.append(station[\"lat\"])\n",
    "                lons.append(station[\"lon\"])\n",
    "                color =\"royalblue\"\n",
    "               \n",
    "                if tag is True:\n",
    "                    color = \"red\" if station.get(\"outlier\",\"False\") ==\"True\"  else \"seagreen\"\n",
    "                    \n",
    "                colors.append(color)\n",
    "\n",
    "                labels.append(str(round(station[\"temp\"],1)))\n",
    "                s_id = station[\"id\"]\n",
    "                if (color == \"red\"):\n",
    "                    s_id = s_id + \"(Avg = \" + str(station[\"avg\"]) +\")\"\n",
    "                ids.append(s_id)\n",
    "\n",
    "            n = len(stations)\n",
    "           \n",
    "            plotly_map_stream.write(dict(\n",
    "                    lon=lons, lat=lats,\n",
    "                    text=labels, hovertext=ids,type=\"Scattermapbox\" , \n",
    "                    marker={\"opacity\":[1.0]*n,\"size\": [30]*n, \"color\":colors,\"symbol\": [\"circle\"] * n}) )\n",
    "            time.sleep(0.5)\n",
    "            if (i % 5) == 0:\n",
    "                sys.stdout.write(\".\")\n",
    "            plotly_map_stream.heartbeat()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    plotly_map_stream.close()\n",
    "    streams_view.stop_data_fetch()\n",
    "    print(\"\\nDone refreshing map. Re-run this cell to send data to the map again\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the map in the notebook\n",
    "\n",
    "We embed the map in the notebook. But no data will display because we have not yet called the  `send_data_to_plotly()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ndsilvaplotly/15.embed\" height=\"800\" width=\"85%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotly.tools.embed(map_url,width=\"85%\",height=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send tagged station data from Streams to the map\n",
    "Call the `send_data_to_plotly()` function. Scroll up to view the map.  You can re-run the cell to start generating data once it stops.\n",
    "This cell will run for a few minutes and then stop.\n",
    "Re-run the cell to continue updating the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending data to map application. Scroll up to view embedded map\n",
      "..."
     ]
    }
   ],
   "source": [
    "print(\"Sending data to map application. Scroll up to view embedded map\")\n",
    "num_refreshes = 30\n",
    "send_data_to_plotly(station_data_view, plotly_data_stream, num_refreshes, tag=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 4: Shutdown\n",
    "1. Kill the simulator process. By default, it will terminate after 30 minutes.\n",
    "2. Terminate the Streams application by running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/src/conda3_runtime.v19/4.1.1/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/src/conda3_runtime.v19/4.1.1/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/fs01/user/s623-f8b248de4f938e-3d6cff768616/.local/lib/python3.5/site-packages/streamsx/rest_primitives.py\", line 162, in __call__\n",
      "    _items = self._get_deduplicated_view_items() or []\n",
      "  File \"/gpfs/fs01/user/s623-f8b248de4f938e-3d6cff768616/.local/lib/python3.5/site-packages/streamsx/rest_primitives.py\", line 170, in _get_deduplicated_view_items\n",
      "    items = self.view.get_view_items()\n",
      "  File \"/gpfs/fs01/user/s623-f8b248de4f938e-3d6cff768616/.local/lib/python3.5/site-packages/streamsx/rest_primitives.py\", line 322, in get_view_items\n",
      "    in self.rest_client.make_request(self.viewItems)['viewItems']]\n",
      "KeyError: 'viewItems'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancelled the Streams application. Run the cells in step 2 to re-submit the application.\n"
     ]
    }
   ],
   "source": [
    "job = job_submission_result.job\n",
    "if job.cancel():\n",
    "    print(\"Cancelled the Streams application. Run the cells in step 2 to re-submit the application.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (Experimental) with Spark 2.0",
   "language": "python",
   "name": "python3-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
